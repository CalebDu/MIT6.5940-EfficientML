{"cells":[{"cell_type":"markdown","metadata":{"id":"PFbUpH8elWQ7"},"source":["# **MIT 6.5940 EfficientML.ai Fall 2023: Lab 0 PyTorch Tutorial**"]},{"cell_type":"markdown","metadata":{"id":"aoNr0MWd5e5m"},"source":["In this tutorial, we will explore how to train a neural network with PyTorch."]},{"cell_type":"markdown","metadata":{"id":"yoBtxdvR5lwM"},"source":["### Setup"]},{"cell_type":"markdown","metadata":{"id":"0oLGv2RjLYh2"},"source":["We will first install a few packages that will be used in this tutorial:"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"3r7Sl2cG7nZF"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Keyring is skipped due to an exception: Failed to unlock the collection!\u001b[0m\n"]}],"source":["!pip install torchprofile 1>/dev/null"]},{"cell_type":"markdown","metadata":{"id":"LYgp0au_LeAd"},"source":["We will then import a few libraries:"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"I3uAhaCSlFrK"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'torchprofile'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-f839a6e5226a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchprofile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprofile_macs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchprofile'"]}],"source":["import random\n","from collections import OrderedDict, defaultdict\n","\n","import numpy as np\n","import torch\n","from matplotlib import pyplot as plt\n","from torch import nn\n","from torch.optim import *\n","from torch.optim.lr_scheduler import *\n","from torch.utils.data import DataLoader\n","from torchprofile import profile_macs\n","from torchvision.datasets import *\n","from torchvision.transforms import *\n","from tqdm.auto import tqdm"]},{"cell_type":"markdown","metadata":{"id":"u1Yx0rDUK5fx"},"source":["To ensure the reproducibility, we will control the seed of random generators:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j_l1wEdeHOlu"},"outputs":[],"source":["random.seed(0)\n","np.random.seed(0)\n","torch.manual_seed(0)"]},{"cell_type":"markdown","metadata":{"id":"u7Y0sLyajGAu"},"source":["### Data"]},{"cell_type":"markdown","metadata":{"id":"VAbL_li0KPsz"},"source":["In this tutorial, we will use CIFAR-10 as our target dataset. This dataset contains images from 10 classes, where each image is of\n","size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pqhy8EJSjJfp"},"outputs":[],"source":["transforms = {\n","  \"train\": Compose([\n","    RandomCrop(32, padding=4),\n","    RandomHorizontalFlip(),\n","    ToTensor(),\n","  ]),\n","  \"test\": ToTensor(),\n","}\n","\n","dataset = {}\n","for split in [\"train\", \"test\"]:\n","  dataset[split] = CIFAR10(\n","    root=\"data/cifar10\",\n","    train=(split == \"train\"),\n","    download=True,\n","    transform=transforms[split],\n","  )"]},{"cell_type":"markdown","metadata":{"id":"ft9wv-tIMUgl"},"source":["We can visualize a few images in the dataset and their corresponding class labels:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ofwgqYb2qsd2"},"outputs":[],"source":["samples = [[] for _ in range(10)]\n","for image, label in dataset[\"test\"]:\n","  if len(samples[label]) < 4:\n","    samples[label].append(image)\n","\n","plt.figure(figsize=(20, 9))\n","for index in range(40):\n","  label = index % 10\n","  image = samples[label][index // 10]\n","\n","  # Convert from CHW to HWC for visualization\n","  image = image.permute(1, 2, 0)\n","\n","  # Convert from class index to class name\n","  label = dataset[\"test\"].classes[label]\n","\n","  # Visualize the image\n","  plt.subplot(4, 10, index + 1)\n","  plt.imshow(image)\n","  plt.title(label)\n","  plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"jkigVqADNeIN"},"source":["To train a neural network, we will need to feed data in batches. We create data loaders with batch size of 512:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4axnQCtnks_s"},"outputs":[],"source":["dataflow = {}\n","for split in ['train', 'test']:\n","  dataflow[split] = DataLoader(\n","    dataset[split],\n","    batch_size=512,\n","    shuffle=(split == 'train'),\n","    num_workers=0,\n","    pin_memory=True,\n","  )"]},{"cell_type":"markdown","metadata":{"id":"_5G1Lf6hOLGT"},"source":["We can print the data type and shape from the training data loader:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ReP2g9pD6ppI"},"outputs":[],"source":["for inputs, targets in dataflow[\"train\"]:\n","  print(\"[inputs] dtype: {}, shape: {}\".format(inputs.dtype, inputs.shape))\n","  print(\"[targets] dtype: {}, shape: {}\".format(targets.dtype, targets.shape))\n","  break"]},{"cell_type":"markdown","metadata":{"id":"sPAEVnixjwb7"},"source":["### Model"]},{"cell_type":"markdown","metadata":{"id":"rFr1Js3-e3rJ"},"source":["In this tutorial, we will use a variant of [VGG-11](https://arxiv.org/abs/1409.1556) (with fewer downsamples and a smaller classifier) as our model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SNLdS_UQjyBf"},"outputs":[],"source":["class VGG(nn.Module):\n","  ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n","\n","  def __init__(self) -> None:\n","    super().__init__()\n","\n","    layers = []\n","    counts = defaultdict(int)\n","\n","    def add(name: str, layer: nn.Module) -> None:\n","      layers.append((f\"{name}{counts[name]}\", layer))\n","      counts[name] += 1\n","\n","    in_channels = 3\n","    for x in self.ARCH:\n","      if x != 'M':\n","        # conv-bn-relu\n","        add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n","        add(\"bn\", nn.BatchNorm2d(x))\n","        add(\"relu\", nn.ReLU(True))\n","        in_channels = x\n","      else:\n","        # maxpool\n","        add(\"pool\", nn.MaxPool2d(2))\n","\n","    self.backbone = nn.Sequential(OrderedDict(layers))\n","    self.classifier = nn.Linear(512, 10)\n","\n","  def forward(self, x: torch.Tensor) -> torch.Tensor:\n","    # backbone: [N, 3, 32, 32] => [N, 512, 2, 2]\n","    x = self.backbone(x)\n","\n","    # avgpool: [N, 512, 2, 2] => [N, 512]\n","    x = x.mean([2, 3])\n","\n","    # classifier: [N, 512] => [N, 10]\n","    x = self.classifier(x)\n","    return x\n","\n","model = VGG().cuda()"]},{"cell_type":"markdown","metadata":{"id":"x19LMKQYw0DI"},"source":["Its backbone is composed of eight `conv-bn-relu` blocks interleaved with four `maxpool`'s to downsample the feature map by 2^4 = 16 times:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BUWmYS2owzCe"},"outputs":[],"source":["print(model.backbone)"]},{"cell_type":"markdown","metadata":{"id":"mApr58LmyDqr"},"source":["After the feature map is pooled, its classifier predicts the final output with a linear layer:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S1GoSsh_yJgD"},"outputs":[],"source":["print(model.classifier)"]},{"cell_type":"markdown","metadata":{"id":"F_RcCWoQ8Kp1"},"source":["As this course focuses on efficiency, we will then inspect its model size and (theoretical) computation cost.\n"]},{"cell_type":"markdown","metadata":{"id":"Zd4Xu-vMyz39"},"source":["* The model size can be estimated by the number of trainable parameters:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4gTfqC0B7Uzi"},"outputs":[],"source":["num_params = 0\n","for param in model.parameters():\n","  if param.requires_grad:\n","    num_params += param.numel()\n","print(\"#Params:\", num_params)"]},{"cell_type":"markdown","metadata":{"id":"uAZoIKIbzLa4"},"source":["* The computation cost can be estimated by the number of [multiply–accumulate operations (MACs)](https://en.wikipedia.org/wiki/Multiply–accumulate_operation) using [TorchProfile](https://github.com/zhijian-liu/torchprofile):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OKVmyWCN7qpp"},"outputs":[],"source":["num_macs = profile_macs(model, torch.zeros(1, 3, 32, 32).cuda())\n","print(\"#MACs:\", num_macs)"]},{"cell_type":"markdown","metadata":{"id":"OYkqpfejzxwq"},"source":["This model has 9.2M parameters and requires 606M MACs for inference. We will work together in the next few labs to improve its efficiency."]},{"cell_type":"markdown","metadata":{"id":"gjDsY9_KkIjZ"},"source":["### Optimization"]},{"cell_type":"markdown","metadata":{"id":"oRg_5KeKLHPj"},"source":["As we are working on a classification problem, we will apply [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) as our loss function to optimize the model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-K0DEhGKkKfF"},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()"]},{"cell_type":"markdown","metadata":{"id":"3H8YniYeLIdg"},"source":["Optimization will be carried out using [stochastic gradient descent (SGD)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) with [momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HXANib83LATH"},"outputs":[],"source":["optimizer = SGD(\n","  model.parameters(),\n","  lr=0.4,\n","  momentum=0.9,\n","  weight_decay=5e-4,\n",")"]},{"cell_type":"markdown","metadata":{"id":"v9X8SiWYLJw2"},"source":["The learning rate will be modulated using the following scheduler (which is adapted from [this blog series](https://myrtle.ai/learn/how-to-train-your-resnet/)):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8mJU5aw8KrVX"},"outputs":[],"source":["num_epochs = 20\n","steps_per_epoch = len(dataflow[\"train\"])\n","\n","# Define the piecewise linear scheduler\n","lr_lambda = lambda step: np.interp(\n","  [step / steps_per_epoch],\n","  [0, num_epochs * 0.3, num_epochs],\n","  [0, 1, 0]\n",")[0]\n","\n","# Visualize the learning rate schedule\n","steps = np.arange(steps_per_epoch * num_epochs)\n","plt.plot(steps, [lr_lambda(step) * 0.4 for step in steps])\n","plt.xlabel(\"Number of Steps\")\n","plt.ylabel(\"Learning Rate\")\n","plt.grid(\"on\")\n","plt.show()\n","\n","scheduler = LambdaLR(optimizer, lr_lambda)"]},{"cell_type":"markdown","metadata":{"id":"i2UFRbRYly50"},"source":["### Training"]},{"cell_type":"markdown","metadata":{"id":"IpHZJpjR7Wy3"},"source":["We first define the training function that optimizes the model for one epoch (*i.e.*, a pass over the training set):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"79GKx_oVl09b"},"outputs":[],"source":["def train(\n","  model: nn.Module,\n","  dataflow: DataLoader,\n","  criterion: nn.Module,\n","  optimizer: Optimizer,\n","  scheduler: LambdaLR,\n",") -> None:\n","  model.train()\n","\n","  for inputs, targets in tqdm(dataflow, desc='train', leave=False):\n","    # Move the data from CPU to GPU\n","    inputs = inputs.cuda()\n","    targets = targets.cuda()\n","\n","    # Reset the gradients (from the last iteration)\n","    optimizer.zero_grad()\n","\n","    # Forward inference\n","    outputs = model(inputs)\n","    loss = criterion(outputs, targets)\n","\n","    # Backward propagation\n","    loss.backward()\n","\n","    # Update optimizer and LR scheduler\n","    optimizer.step()\n","    scheduler.step()"]},{"cell_type":"markdown","metadata":{"id":"SwAbMdUq7YrE"},"source":["We then define the evaluation function that calculates the metric (*i.e.*, accuracy in our case) on the test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"44_AriMP4G_A"},"outputs":[],"source":["@torch.inference_mode()\n","def evaluate(\n","  model: nn.Module,\n","  dataflow: DataLoader\n",") -> float:\n","  model.eval()\n","\n","  num_samples = 0\n","  num_correct = 0\n","\n","  for inputs, targets in tqdm(dataflow, desc=\"eval\", leave=False):\n","    # Move the data from CPU to GPU\n","    inputs = inputs.cuda()\n","    targets = targets.cuda()\n","\n","    # Inference\n","    outputs = model(inputs)\n","\n","    # Convert logits to class indices\n","    outputs = outputs.argmax(dim=1)\n","\n","    # Update metrics\n","    num_samples += targets.size(0)\n","    num_correct += (outputs == targets).sum()\n","\n","  return (num_correct / num_samples * 100).item()"]},{"cell_type":"markdown","metadata":{"id":"6XSU9oFD7aXs"},"source":["With training and evaluation functions, we can finally start training the model! This will take around 10 minutes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4iWaYpw_4E8f"},"outputs":[],"source":["for epoch_num in tqdm(range(1, num_epochs + 1)):\n","  train(model, dataflow[\"train\"], criterion, optimizer, scheduler)\n","  metric = evaluate(model, dataflow[\"test\"])\n","  print(f\"epoch {epoch_num}:\", metric)"]},{"cell_type":"markdown","metadata":{"id":"96XTuBat-e-d"},"source":["If everything goes well, your trained model should be able to achieve >92.5\\% of accuracy!"]},{"cell_type":"markdown","metadata":{"id":"ck6iME0rLjuk"},"source":["### Visualization"]},{"cell_type":"markdown","metadata":{"id":"-8rVV1SOSUsR"},"source":["We can visualize the model's prediction to see how the model truly performs:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"inwZEfX3Mo6A"},"outputs":[],"source":["plt.figure(figsize=(20, 10))\n","for index in range(40):\n","  image, label = dataset[\"test\"][index]\n","\n","  # Model inference\n","  model.eval()\n","  with torch.inference_mode():\n","    pred = model(image.unsqueeze(dim=0).cuda())\n","    pred = pred.argmax(dim=1)\n","\n","  # Convert from CHW to HWC for visualization\n","  image = image.permute(1, 2, 0)\n","\n","  # Convert from class indices to class names\n","  pred = dataset[\"test\"].classes[pred]\n","  label = dataset[\"test\"].classes[label]\n","\n","  # Visualize the image\n","  plt.subplot(4, 10, index + 1)\n","  plt.imshow(image)\n","  plt.title(f\"pred: {pred}\" + \"\\n\" + f\"label: {label}\")\n","  plt.axis(\"off\")\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}
